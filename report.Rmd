---
title: "Assessing feature importance for sgRNA using supervised machine learning"
author: "Emil Carlstedt, Alec Thomsen"
date: "2025-05-29"
output: html_document
bibliography: references.bib
csl: harvard-limerick.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(patchwork)
library(knitr)
library(kableExtra)
load("saved_vars.RData")
```
## Introduction
The discovery of CRISPR-Cas9 has led to innumerable possibilities in the field of biotechnology. One of them involves the use of single-guide RNA engineered to target specific genes of interest in order to do knock-out experiments. Understanding how to design these sgRNAs is crucial to the experiments success, therefore guide lines have been developed. Although the guidelines differ for different species they have been shown to be effective.[@schindele2020crispr] Our interest is to find out just which of the features are most important for the efficiency of the sgRNA.
To tackle this issue we will employ a tactic involving supervised machine learning. This will help us assess what features of a sgRNA are most important for its effectiveness.

## Theory
The sgRNA dataset we chose to use was of the GeCKO v2 sgRNA library, and it was designed to give high-throughput genome wide knockout screening
Therefore when using this data we assume that these sgRNA's reliably bind to their respective target binding site, and bind with high accuracy.

## Implementation
1. Flowchart or overview of the solution; link to GitHub  
2. Relevant Screenshots of the RShiny solution  
3. Bonus material - additional tasks you added that you want to share with the others  

### Pre-Processing
The features of interest; nucleotide position, gc content, exon position, and RNA sequencing expression were extracted from the database. NA values were omitted and RNA-seq expression was log2 transformed for easier interpretation. Only the sgRNA with RNA-seq expression above 1 were kept as they interpreted as actively transcribed genes.
An initial look at the distribution of the features was done using a box plot (See *Figure 1*). This identified a skewed distribution of the data, especially in the feature exon position.

```{r, echo = FALSE, fig.align='center', fig.width=10, fig.height=5, fig.cap = "Figure 1. Boxplots displaying the distribution of selected features (exon_position, gc_content, and RNAseq_expression). The plot to the left displays the data pre filtering and the one on the right after filtering. Looking specifically on the boxplot for exon_position we can see that there are a outliers that skews the data, this is adress by the removal of the outliers."}
boxplot_pf <- readRDS("boxplot_pf")
boxplot_af <- readRDS("boxplot_af")
print(boxplot_pf + boxplot_af)
```

We then filtered the data, only keeping those with a negative LFC. We then used the absolute value of the LFC as our dependent variable for the model.  
The data was split into training and test sets using a 80/20 split. Using the hyper parameters seen in *Table 1*, the model underwent a 5-fold cross-validation, for 1000 rounds with early stopping after 10, in order to find the best boosting round.  

```{r, echo = FALSE}
param_df <- data.frame(Parameter = names(params), Value = unlist(params), row.names = NULL)

kable(param_df, caption = "Table 1. Model Parameters") %>%
  kable_styling(full_width = FALSE, position = "left") %>%
  column_spec(1, width = "4cm") %>%
  column_spec(2, width = "4cm")    

```
  
This resulted in a optimal boosting round of `r best_nrounds` which was implemented when training the model on the training data.  
After the training SHAP values were calculated as seen in *Figure 2*. Here we can see that highly expressed genes are positively related to a large LFC value, these genes could possible be viewed as "householding" genes and have a vital part in the cell. Nucleotide position has unclear results for example position 20 where all nucleotides have a negative impact on the model which goes against previous studies saying that the later positions are more important.

```{r, echo = FALSE, fig.cap = "Figure 2. SHAP values for the final model. RNA-seq expression can be seen as the highest contributing feature to the models prediction. Higher RNA expression leads to higher LFC."}
shap_plot <- readRDS("shap_final")
print(shap_plot)
```

After training the model was used to predict the LFC on the testing data that was withheld from training. The RMSE and MAE were not that large but the R2 was very low, meaning the model can make semi accurate predictions but is unable to explain the variance in the data.(See *Table 2*).

```{r, echo = FALSE}
kable(eval_df, caption = "Table 2. Evaluation Metrics for Final Model.") %>%
  kable_styling(full_width = FALSE, position = "left") %>%
  column_spec(1, width = "6cm") %>%
  column_spec(2, width = "4cm")
```

This can also be seen when looking at the Prediction vs Actual plot and Predicted vs Residuals (See *Figure 3 and 4*). The model seems limited in its predictions to a range of 0.5 to 1.5 regardless of the actual value of the sequence. And the fanning pattern displayed in *Figure 4* indicates heteroscadescity in the models predictions as the error is increasing when the model tries to predict larger values.

```{r, echo = FALSE, fig.cap = "Figure 3 (left) displaying a pattern that the model can only predict values between 0.5 and 1.5 regardless of the actual value. Figure 4 (right) Displaying a fanning pattern, suggesting heteroscedascity."}
predicted_vs_actual <- readRDS("predicted_vs_actual")
predicted_vs_residuals <- readRDS("predicted_vs_residuals")
print(predicted_vs_actual + predicted_vs_residuals)
```

## Evaluation

Our model is unable to find a pattern in the data we trained it on and therefore the SHAP values are not trustworthy. No conclusions towards our original goal can be drawn based on our results.  
This is most likely to the lack of larger scores in the data as seen in distribution of LFC(See Figure 1A in appendix).  
Testing different models and different targets could also be an option for further testing if more time was available.
Using LFC as a target might not have been the best choice as we would only get on successful knock outs that targeted a vital gene. We tried to work around this by filtering out LFCs that were not of interest but this also means that we loose a lot of data. Using transcriptomic data for creating a classification on successful knock outs would result in minimum loss of data as even non vital genes would be included. Of course there are other difficulties with that approach. Would you use a binary approach saying that if its below a baseline with a certain amount its regarded a knock out or do you create different levels of success based on how much the transcription is reduced. And how do you decide on these limits? A lot of questions arise once you start digging into this subject and if time allows we would like to explore them all.

## Appendix
Figure 1A
```{r, echo = FALSE}
dist_lfc_pf <- readRDS("dist_lfc_pf")
dist_lfc_af <- readRDS("dist_lfc_af")
print(dist_lfc_pf + dist_lfc_af)
```

## References